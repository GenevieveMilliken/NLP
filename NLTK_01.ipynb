{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcvNPPMI1C49FZ6TJSCrk4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GenevieveMilliken/NLP/blob/main/NLTK_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Toolkit (NLTK)\n",
        "\n",
        "Notes from Ch. 1: https://www.nltk.org/book/ch01.html\n",
        "\n",
        "The Natural Language Toolkit, or more commonly NLTK, is a Python suite of libraries used for natural language processing (NLP) on English-language texts.\n",
        "\n",
        "NLTK is intended to support research and teaching in NLP or closely related areas, including empirical linguistics, cognitive science, artificial intelligence, information retrieval, and machine learning.\n",
        "\n",
        "NLTK supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities."
      ],
      "metadata": {
        "id": "r3QbLNjBX_fx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started"
      ],
      "metadata": {
        "id": "2sFg6Rh37jTj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRIh4OKXVkKO"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "uEHiHJ1rVzfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.download()"
      ],
      "metadata": {
        "id": "KnX8cg5Au6w1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('book')"
      ],
      "metadata": {
        "id": "w4QhhFdkV2p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.book import *"
      ],
      "metadata": {
        "id": "lDW9kt8-Whp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Searching Texts"
      ],
      "metadata": {
        "id": "CwAMGeZF7u5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK provides the function `concordance()` to locate and print series of phrases that contain the keyword. However, the function only print the output. "
      ],
      "metadata": {
        "id": "109PAXmiaKSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1.concordance(\"monstrous\")"
      ],
      "metadata": {
        "id": "FQDcDvXDXNCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1.concordance(\"flood\")"
      ],
      "metadata": {
        "id": "unQ6g0HfXfwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1.concordance(\"Whale\")"
      ],
      "metadata": {
        "id": "K81QoqWMXqft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2.concordance(\"affection\")"
      ],
      "metadata": {
        "id": "be6-dznsZzVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A concordance permits us to see words in context. For example, we saw that *monstrous* occurred in contexts such as *...most monstrous size* and *...monstrous clubs and spears*. What other words appear in a similar range of contexts? We can find out by appending the term similar to the name of the text in `question`, then inserting the relevant word in parentheses:"
      ],
      "metadata": {
        "id": "VtzZt9cMaHpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1.similar(\"monstrous\")"
      ],
      "metadata": {
        "id": "vjcfB3HGa3vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2.similar(\"monstrous\")"
      ],
      "metadata": {
        "id": "hn7fDk5HZ5HA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2.common_contexts([\"monstrous\", \"remarkably\"])"
      ],
      "metadata": {
        "id": "q4kXRJNWcLC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is one thing to automatically detect that a particular word occurs in a text, and to display some words that appear in the same context. However, we can also determine the location of a word in the text: how many words from the beginning it appears. This positional information can be displayed using a dispersion plot."
      ],
      "metadata": {
        "id": "WpLn0PbI4Nmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dispersion plot for Inaugural Address Corpus \n",
        "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
      ],
      "metadata": {
        "id": "9AXGU_6n4BTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, just for fun, let's try generating some random text in the various styles we have just seen."
      ],
      "metadata": {
        "id": "WjbSxRkH7Wex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text2.generate()"
      ],
      "metadata": {
        "id": "BeYj3-Jr4sGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Counting Vocabulary\n",
        "The most obvious fact about texts that emerges from the preceding examples is that they differ in the vocabulary they use. In this section we will see how to use the computer to count the words in a text in a variety of useful ways"
      ],
      "metadata": {
        "id": "ZVAn2X487aa6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin by finding out the length of a text from start to finish, in terms of the words and punctuation symbols that appear. We use the term len to get the length of something, which we'll apply here to the book of Genesis:"
      ],
      "metadata": {
        "id": "OWc6-ArI8Zyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(text3)\n",
        "print(f\"The word count of {text3.name} is {len(text3)}.\")"
      ],
      "metadata": {
        "id": "oNPaudMV79-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So Genesis has 44,764 words and punctuation symbols, or \"tokens.\" A token is the technical name for a sequence of characters."
      ],
      "metadata": {
        "id": "7aCO77dcAnO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we count the number of tokens in a text, say, the phrase *to be or not to be*, we are counting occurrences of these sequences. Thus, in our example phrase there are two occurrences of *to*, two of *be*, and one each of *or* and *not*. But there are only four distinct vocabulary items in this phrase. **How many distinct words does the book of Genesis contain?**\n",
        "\n",
        "The vocabulary of a text is just the [set](https://www.w3schools.com/python/python_sets.asp) of tokens that it uses, since in a set, all duplicates are collapsed together. In Python we can obtain the vocabulary items of text3 with the command: set(text3). "
      ],
      "metadata": {
        "id": "omjxvI6jBcpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(set(text3))"
      ],
      "metadata": {
        "id": "nxoV7nUhAmJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(text3))\n",
        "\n",
        "print(f\"{text3.name} has {len(set(text3))} distinct words or \\\"item types\\\".\")"
      ],
      "metadata": {
        "id": "H45MG0YfAU_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A word type is the form or spelling of the word independently of its specific occurrences in a text — that is, the word considered as a unique item of vocabulary. Our count of 2,789 items will include punctuation symbols, so we will generally call these unique items types instead of word types.\n",
        "\n",
        "Now, let's calculate a measure of the lexical richness of the text."
      ],
      "metadata": {
        "id": "vkvVLMNUD94K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(text3)) / len(text3)"
      ],
      "metadata": {
        "id": "36NZW5vqFSix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next example shows us that the number of distinct words is just 6% of the total number of words, or equivalently that each word is used 16 times on average"
      ],
      "metadata": {
        "id": "g-6h6nojFxJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " 100 / 6"
      ],
      "metadata": {
        "id": "_nC2ihYkFveN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's focus on particular words. We can count how often a word occurs in a text, and compute what percentage of the text is taken up by a specific word:"
      ],
      "metadata": {
        "id": "xycVpYXwGLSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1.count(\"boat\")\n",
        "\n",
        "100 * text1.count(\"boat\") / len(text1)"
      ],
      "metadata": {
        "id": "Aq1MZEOdGEaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if might become tedious to do this for every word, so we can define a function \n",
        "\n",
        "def lexical_diversity(text):\n",
        "  return len(set(text)) / len(text)\n",
        "\n",
        "def percentage(count, total):\n",
        "  return 100 * count / total\n"
      ],
      "metadata": {
        "id": "cQZOShkIHY3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lexical_diversity(text3)"
      ],
      "metadata": {
        "id": "Y8nYS0qpIH85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percentage(text1.count(\"boat\") , len(text1))"
      ],
      "metadata": {
        "id": "aPCg95SNIUVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  A Closer Look at Python: Texts as Lists of Words"
      ],
      "metadata": {
        "id": "kOF2CleEJEuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK has converted the first sentence of each of the books into a list. "
      ],
      "metadata": {
        "id": "vMHDWw86KSmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent1"
      ],
      "metadata": {
        "id": "ODHJf3cYKRer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent2"
      ],
      "metadata": {
        "id": "_qObRWhYKAgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent3"
      ],
      "metadata": {
        "id": "xlfSksE1Ke7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have seen, a text in Python is a list of words, represented using a combination of brackets and quotes. Just as with an ordinary page of text, we can count up the total number of words in text1 with len(text1), and count the occurrences in a text of a particular word — say, 'heaven' — using text1.count('heaven')."
      ],
      "metadata": {
        "id": "Dsa7Zjs5LfDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(text1)\n",
        "\n",
        "text1.count(\"Heaven\")"
      ],
      "metadata": {
        "id": "gxxuFXwRLsDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using index to get the 598th word \n",
        "\n",
        "text1[599]"
      ],
      "metadata": {
        "id": "HQ2FW6UyL-Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# index where the word first occurs\n",
        "text1.index(\"who\")"
      ],
      "metadata": {
        "id": "ajodVsq8MKC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also slice \n",
        "# first 100 items of Moby Dick\n",
        "\n",
        "text1[0:100]"
      ],
      "metadata": {
        "id": "ZrYluLXMMf4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''rest of this section in the documentation covers strings, concatination, index, slices;\n",
        " covered in NYUHSL Intro Python class'''"
      ],
      "metadata": {
        "id": "cDXGBfmVg4Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing with Language: Simple Statistic"
      ],
      "metadata": {
        "id": "aiKbobU4hbtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Review\n",
        " \t\n",
        "saying = ['After', 'all', 'is', 'said', 'and', 'done', 'more', 'is', 'said', 'than', 'done']\n",
        "tokens = set(saying)\n",
        "tokens = sorted(tokens)\n",
        "tokens[-2:]"
      ],
      "metadata": {
        "id": "qADt5cE8hVWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frequency Distributions\n",
        "\n",
        "A frequency distribution tells us the frequency of each vocabulary item in the text. It is a \"distribution\" because it tells us how the total number of word tokens in the text are distributed across the vocabulary items. Since we often need frequency distributions in language processing, NLTK provides built-in support for them. Let's use a FreqDist to find the 50 most frequent words of Moby Dick:"
      ],
      "metadata": {
        "id": "6uauDYX2h-xL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dist_1 = FreqDist(text1)\n",
        "print(freq_dist_1)\n",
        "\n",
        "# type(freq_dist_1)\n",
        "\n",
        "freq_dist_1.most_common(50)"
      ],
      "metadata": {
        "id": "whuV7Lrkh4Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do any words produced in the last example help us grasp the topic or genre of this text? Only one word, whale, is slightly informative! It occurs over 900 times. The rest of the words tell us nothing about the text; they're just English \"plumbing.\" What proportion of the text is taken up with such words? We can generate a cumulative frequency plot for these words, using fdist1.plot(50, cumulative=True), to produce the graph in 3.2. These 50 words account for nearly half the book!"
      ],
      "metadata": {
        "id": "wOPsKRn7jhkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dist_1.plot(50, cumulative=True)"
      ],
      "metadata": {
        "id": "ba6gzneQjXN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-grained Selection of Words\n",
        "\n",
        "Next, let's look at the long words of a text; perhaps these will be more characteristic and informative. For this we adapt some notation from set theory. We would like to find the words from the vocabulary of the text that are more than 15 characters long. Let's call this property P, so that P(w) is true if and only if w is more than 15 characters long. Now we can express the words of interest using mathematical set notation as shown in (1a). This means \"the set of all w such that w is an element of V (the vocabulary) and w has property P\". 2b corresponding Python expression in a list comprehension. \n",
        "\n",
        "1a. `{w | w ∈ V & P(w)}` <br>\n",
        "2b.   `[w for w in V if p(w)]`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\t\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5muqwRwyj3xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " V = set(text1)\n",
        "\n",
        " long_words = [w for w in V if len(w) > 15]\n",
        " print(sorted(long_words))"
      ],
      "metadata": {
        "id": "1Z2hATfZj02M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These very long words are often hapaxes (i.e., unique) and perhaps it would be better to find frequently occurring long words. This seems promising since it eliminates frequent short words (e.g., the) and infrequent long words (e.g. antiphilosophists). Here are all words from the chat corpus that are longer than seven characters, that occur more than seven times:"
      ],
      "metadata": {
        "id": "bDqi-Y1uqPtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dist_5 = FreqDist(text5)\n",
        "sorted(w for w in set(text5) if len(w) > 7 and freq_dist_5[w] > 7)"
      ],
      "metadata": {
        "id": "Sb8WSHLlqG8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collocations and Bigrams\n",
        "\n",
        "A collocation is a sequence of words that occur together unusually often. Thus *red wine* is a collocation, whereas *the wine* is not. A characteristic of collocations is that they are resistant to substitution with words that have similar senses; for example, maroon wine sounds definitely odd.\n",
        "\n",
        "To get a handle on collocations, we start off by extracting from a text a list of word pairs, also known as bigrams. This is easily accomplished with the function bigrams():"
      ],
      "metadata": {
        "id": "4Z4pCHU0qwC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(bigrams([\"more\", \"is\", \"said\", \"than\", \"done\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq3Q7hl5qtuk",
        "outputId": "25ccc85f-07cf-4dd0-e4f0-8d6a29d1b421"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('more', 'is'), ('is', 'said'), ('said', 'than'), ('than', 'done')]"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, collocations are essentially just frequent bigrams, except that we want to pay more attention to the cases that involve rare words. In particular, we want to find bigrams that occur more often than we would expect based on the frequency of the individual words. The collocations() function does this for us. "
      ],
      "metadata": {
        "id": "JhMSTmvGwGIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text4.collocations()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIHvldh3v9oq",
        "outputId": "49119ef7-fef9-4eaa-e486-8d32b1dfb301"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "United States; fellow citizens; years ago; four years; Federal\n",
            "Government; General Government; American people; Vice President; God\n",
            "bless; Chief Justice; one another; fellow Americans; Old World;\n",
            "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
            "tribes; public debt; foreign nations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2.collocations()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mG7jbuCwTil",
        "outputId": "13567356-8531-4793-ebac-a438aafda5b1"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colonel Brandon; Sir John; Lady Middleton; Miss Dashwood; every thing;\n",
            "thousand pounds; dare say; Miss Steeles; said Elinor; Miss Steele;\n",
            "every body; John Dashwood; great deal; Harley Street; Berkeley Street;\n",
            "Miss Dashwoods; young man; Combe Magna; every day; next morning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1.collocations()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxJrmQwswdPK",
        "outputId": "9aa10ed1-e91d-49e7-e456-04d0beb5c8ec"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; sperm\n",
            "whale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;\n",
            "years ago; lower jaw; never mind; Father Mapple; cried Stubb; chief\n",
            "mate; white whale; ivory leg; one hand\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collocation(text):\n",
        "  return text.collocations()\n",
        "\n",
        "books = [text1, text2, text3, text4, text5, text6, text7, text8, text8]\n",
        "\n",
        "for book in books:\n",
        "  collocation(book)\n",
        "  print(\"-------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNX-FmB7wkAS",
        "outputId": "26675096-8bd9-48f1-b9b4-1d191d1d7358"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; sperm\n",
            "whale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;\n",
            "years ago; lower jaw; never mind; Father Mapple; cried Stubb; chief\n",
            "mate; white whale; ivory leg; one hand\n",
            "-------\n",
            "Colonel Brandon; Sir John; Lady Middleton; Miss Dashwood; every thing;\n",
            "thousand pounds; dare say; Miss Steeles; said Elinor; Miss Steele;\n",
            "every body; John Dashwood; great deal; Harley Street; Berkeley Street;\n",
            "Miss Dashwoods; young man; Combe Magna; every day; next morning\n",
            "-------\n",
            "said unto; pray thee; thou shalt; thou hast; thy seed; years old;\n",
            "spake unto; thou art; LORD God; every living; God hath; begat sons;\n",
            "seven years; shalt thou; little ones; living creature; creeping thing;\n",
            "savoury meat; thirty years; every beast\n",
            "-------\n",
            "United States; fellow citizens; years ago; four years; Federal\n",
            "Government; General Government; American people; Vice President; God\n",
            "bless; Chief Justice; one another; fellow Americans; Old World;\n",
            "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
            "tribes; public debt; foreign nations\n",
            "-------\n",
            "wanna chat; PART JOIN; MODE #14-19teens; JOIN PART; PART PART;\n",
            "cute.-ass MP3; MP3 player; JOIN JOIN; times .. .; ACTION watches; guys\n",
            "wanna; song lasts; last night; ACTION sits; -...)...- S.M.R.; Lime\n",
            "Player; Player 12%; dont know; lez gurls; long time\n",
            "-------\n",
            "BLACK KNIGHT; clop clop; HEAD KNIGHT; mumble mumble; Holy Grail;\n",
            "squeak squeak; FRENCH GUARD; saw saw; Sir Robin; Run away; CARTOON\n",
            "CHARACTER; King Arthur; Iesu domine; Pie Iesu; DEAD PERSON; Round\n",
            "Table; clap clap; OLD MAN; dramatic chord; dona eis\n",
            "-------\n",
            "million *U*; New York; billion *U*; Wall Street; program trading; Mrs.\n",
            "Yeargin; vice president; Stock Exchange; Big Board; Georgia Gulf;\n",
            "chief executive; Dow Jones; S&P 500; says *T*-1; York Stock; last\n",
            "year; Sea Containers; South Korea; American Express; San Francisco\n",
            "-------\n",
            "would like; medium build; social drinker; quiet nights; non smoker;\n",
            "long term; age open; Would like; easy going; financially secure; fun\n",
            "times; similar interests; Age open; weekends away; poss rship; well\n",
            "presented; never married; single mum; permanent relationship; slim\n",
            "build\n",
            "-------\n",
            "would like; medium build; social drinker; quiet nights; non smoker;\n",
            "long term; age open; Would like; easy going; financially secure; fun\n",
            "times; similar interests; Age open; weekends away; poss rship; well\n",
            "presented; never married; single mum; permanent relationship; slim\n",
            "build\n",
            "-------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's use set to get the lengths of the words as a numerical value\n",
        "\n",
        "sorted(set([len(w) for w in text1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddTGOapxyIQT",
        "outputId": "9d321d5b-da7a-456f-fc61-7a6942fc985f"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20]"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# freq distribution \n",
        "\n",
        "freq_dist = FreqDist(len(w) for w in text1)\n",
        "print(freq_dist)\n",
        "freq_dist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa08xJ46y0J8",
        "outputId": "d48bbfc4-0156-4091-fda8-4a2c44e83cc9"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<FreqDist with 19 samples and 260819 outcomes>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({3: 50223, 1: 47933, 4: 42345, 2: 38513, 5: 26597, 6: 17111, 7: 14399, 8: 9966, 9: 6428, 10: 3528, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dist.most_common()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWEZqNEYznpP",
        "outputId": "1572f007-f8a9-48c1-cd8f-2feb383db83a"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3, 50223),\n",
              " (1, 47933),\n",
              " (4, 42345),\n",
              " (2, 38513),\n",
              " (5, 26597),\n",
              " (6, 17111),\n",
              " (7, 14399),\n",
              " (8, 9966),\n",
              " (9, 6428),\n",
              " (10, 3528),\n",
              " (11, 1873),\n",
              " (12, 1053),\n",
              " (13, 567),\n",
              " (14, 177),\n",
              " (15, 70),\n",
              " (16, 22),\n",
              " (17, 12),\n",
              " (18, 1),\n",
              " (20, 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Python: Making Decisions and Taking Control\n"
      ],
      "metadata": {
        "id": "2f2daNuY0FXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conditionals \n",
        "\n",
        "sent7\n",
        "\n",
        "print([w for w in sent7 if len(w) < 4])\n",
        "print([w for w in sent7 if len(w) <= 4])\n",
        "print([w for w in sent7 if len(w) == 4])\n",
        "print([w for w in sent7 if len(w) != 4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GT0VVe-0Ehy",
        "outputId": "1c40ebb4-87d8-44e8-81d3-eba0d52ae85a"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[',', '61', 'old', ',', 'the', 'as', 'a', '29', '.']\n",
            "[',', '61', 'old', ',', 'will', 'join', 'the', 'as', 'a', 'Nov.', '29', '.']\n",
            "['will', 'join', 'Nov.']\n",
            "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', '29', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(w for w in set(text1) if w.endswith('ableness'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__VOgK730yC2",
        "outputId": "8b714a02-d1fc-4377-e397-c57721952993"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['comfortableness',\n",
              " 'honourableness',\n",
              " 'immutableness',\n",
              " 'indispensableness',\n",
              " 'indomitableness',\n",
              " 'intolerableness',\n",
              " 'palpableness',\n",
              " 'reasonableness',\n",
              " 'uncomfortableness']"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(term for term in set(text4) if 'gnt' in term)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L199r1CI0U9J",
        "outputId": "69bfea2e-8d05-4ad5-a5b0-27f39eaf6ecd"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sovereignty', 'sovereignties', 'sovereignty']"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Looping with Conditions\n",
        "\n",
        "Now we can combine the if and for statements. We will loop over every item of the list, and print the item only if it ends with the letter l. "
      ],
      "metadata": {
        "id": "GwJfyMyM14h6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 \n",
        "\n",
        "for xyz in sent1:\n",
        "  if xyz.endswith(\"l\"):\n",
        "    print(xyz)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeoDjMIF2tV3",
        "outputId": "66ca9806-0786-4f27-8771-f70b18355760"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Call\n",
            "Ishmael\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.sonority_sequencing import punctuation\n",
        "for token in sent1:\n",
        "  if token.islower():\n",
        "    print(f\"{token} is lower case.\")\n",
        "  elif token.istitle():\n",
        "    print(f\"{token} is title case.\")\n",
        "  else: \n",
        "    print(f\"{token} is punctuation\")\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fLnxI_t3Lez",
        "outputId": "39cf00b1-7ae8-48f0-fb89-3c4ad6864116"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Call is title case.\n",
            "me is lower case.\n",
            "Ishmael is title case.\n",
            ". is punctuation\n"
          ]
        }
      ]
    }
  ]
}