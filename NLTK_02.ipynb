{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPP0T+Cdtr94NCGrbPBM9JO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GenevieveMilliken/NLP/blob/main/NLTK_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accessing Text Corpora and Lexical Resources\n",
        "\n",
        "https://www.nltk.org/book/ch02.html\n",
        "\n",
        "1. What are some useful text corpora and lexical \n",
        "resources, and how can we access them with Python?\n",
        "2. Which Python constructs are most helpful for this work?\n",
        "3. How do we avoid repeating ourselves when writing Python code?"
      ],
      "metadata": {
        "id": "82yyTG8RvDvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK includes a small selection of texts from the Project Gutenberg electronic text archive, which contains some 25,000 free electronic books, hosted at http://www.gutenberg.org/. We begin by getting the Python interpreter to load the NLTK package, then ask to see nltk.corpus.gutenberg.fileids(), the file identifiers in this corpus:"
      ],
      "metadata": {
        "id": "EMH6qdAlvu5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Gutenberg Corpus\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"gutenberg\")\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import gutenberg"
      ],
      "metadata": {
        "id": "TgarNGuBvWCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "gutenberg.fileids()\n",
        "\n",
        "for fileid in gutenberg.fileids(): \n",
        "  num_chars = len(gutenberg.raw(fileid))\n",
        "  num_words = len(gutenberg.words(fileid))\n",
        "  num_sents = len(gutenberg.sents(fileid))\n",
        "  num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
        "  print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)\n"
      ],
      "metadata": {
        "id": "nPkVmeP70Ia4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This program displays three statistics for each text: average word length, average sentence length, and the number of times each vocabulary item appears in the text on average (our lexical diversity score). Observe that average word length appears to be a general property of English, since it has a recurrent value of 4. (In fact, the average word length is really 3 not 4, since the num_chars variable counts space characters.) By contrast average sentence length and lexical diversity appear to be characteristics of particular authors."
      ],
      "metadata": {
        "id": "K5qXia3K0Va-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emma = nltk.corpus.gutenberg.words(\"austen-emma.txt\")"
      ],
      "metadata": {
        "id": "2LAuuMRRyVpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTF09l4uvCbt"
      },
      "outputs": [],
      "source": [
        "len(emma)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unlike in chapter 2, we have to turn our corpus into a text object to perform concordance\n",
        "\n",
        "emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
        "emma.concordance(\"melancholy\")"
      ],
      "metadata": {
        "id": "v79sRmEkw5iR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n",
        "len(macbeth_sentences)"
      ],
      "metadata": {
        "id": "Qyj0iLOxyFIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(macbeth_sentences[1906])"
      ],
      "metadata": {
        "id": "0b5kcais00ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "longest_len = max(len(s) for s in macbeth_sentences)\n",
        "longest_len\n",
        "print([s for s in macbeth_sentences if len(s) == longest_len])"
      ],
      "metadata": {
        "id": "5GZli1L805rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List of Corpora\n",
        "\n",
        "The first secord of Ch. 2 provides information on available corpora: \n",
        "* Gutenberg Corpus\n",
        "* Web and Chat Text\n",
        "* Brown Corpus\n",
        "* Reuters Corpus\n",
        "* Inaugural Address Corpus\n",
        "* Annotated Text Corpora\n",
        "* Multi-language ( Universal Declaration of Human Rights)\n",
        "* https://www.nltk.org/howto/corpus.html"
      ],
      "metadata": {
        "id": "NV3ETwzN2XK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(nltk.corpus.reader)"
      ],
      "metadata": {
        "id": "1BBDyc8D3qCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# raw, word, and sent \n",
        "\n",
        "raw = gutenberg.raw(\"burgess-busterbrown.txt\")\n",
        "print(raw[1:20])\n",
        "words = gutenberg.words(\"burgess-busterbrown.txt\")\n",
        "print(words[1:20])\n",
        "sents = gutenberg.sents(\"burgess-busterbrown.txt\")\n",
        "print(sents[1:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6ivCxPr4DQn",
        "outputId": "c92c16a8-bcf9-42ae-c4e2-f2ff4de643fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Adventures of B\n",
            "['The', 'Adventures', 'of', 'Buster', 'Bear', 'by', 'Thornton', 'W', '.', 'Burgess', '1920', ']', 'I', 'BUSTER', 'BEAR', 'GOES', 'FISHING', 'Buster', 'Bear']\n",
            "[['I'], ['BUSTER', 'BEAR', 'GOES', 'FISHING'], ['Buster', 'Bear', 'yawned', 'as', 'he', 'lay', 'on', 'his', 'comfortable', 'bed', 'of', 'leaves', 'and', 'watched', 'the', 'first', 'early', 'morning', 'sunbeams', 'creeping', 'through', 'the', 'Green', 'Forest', 'to', 'chase', 'out', 'the', 'Black', 'Shadows', '.'], ['Once', 'more', 'he', 'yawned', ',', 'and', 'slowly', 'got', 'to', 'his', 'feet', 'and', 'shook', 'himself', '.'], ['Then', 'he', 'walked', 'over', 'to', 'a', 'big', 'pine', '-', 'tree', ',', 'stood', 'up', 'on', 'his', 'hind', 'legs', ',', 'reached', 'as', 'high', 'up', 'on', 'the', 'trunk', 'of', 'the', 'tree', 'as', 'he', 'could', ',', 'and', 'scratched', 'the', 'bark', 'with', 'his', 'great', 'claws', '.'], ['After', 'that', 'he', 'yawned', 'until', 'it', 'seemed', 'as', 'if', 'his', 'jaws', 'would', 'crack', ',', 'and', 'then', 'sat', 'down', 'to', 'think', 'what', 'he', 'wanted', 'for', 'breakfast', '.'], ['While', 'he', 'sat', 'there', ',', 'trying', 'to', 'make', 'up', 'his', 'mind', 'what', 'would', 'taste', 'best', ',', 'he', 'was', 'listening', 'to', 'the', 'sounds', 'that', 'told', 'of', 'the', 'waking', 'of', 'all', 'the', 'little', 'people', 'who', 'live', 'in', 'the', 'Green', 'Forest', '.'], ['He', 'heard', 'Sammy', 'Jay', 'way', 'off', 'in', 'the', 'distance', 'screaming', ',', '\"', 'Thief', '!'], ['Thief', '!\"'], ['and', 'grinned', '.'], ['\"', 'I', 'wonder', ',\"', 'thought', 'Buster', ',', '\"', 'if', 'some', 'one', 'has', 'stolen', 'Sammy', \"'\", 's', 'breakfast', ',', 'or', 'if', 'he', 'has', 'stolen', 'the', 'breakfast', 'of', 'some', 'one', 'else', '.'], ['Probably', 'he', 'is', 'the', 'thief', 'himself', '.\"'], ['He', 'heard', 'Chatterer', 'the', 'Red', 'Squirrel', 'scolding', 'as', 'fast', 'as', 'he', 'could', 'make', 'his', 'tongue', 'go', 'and', 'working', 'himself', 'into', 'a', 'terrible', 'rage', '.'], ['\"', 'Must', 'be', 'that', 'Chatterer', 'got', 'out', 'of', 'bed', 'the', 'wrong', 'way', 'this', 'morning', ',\"', 'thought', 'he', '.'], ['He', 'heard', 'Blacky', 'the', 'Crow', 'cawing', 'at', 'the', 'top', 'of', 'his', 'lungs', ',', 'and', 'he', 'knew', 'by', 'the', 'sound', 'that', 'Blacky', 'was', 'getting', 'into', 'mischief', 'of', 'some', 'kind', '.'], ['He', 'heard', 'the', 'sweet', 'voices', 'of', 'happy', 'little', 'singers', ',', 'and', 'they', 'were', 'good', 'to', 'hear', '.'], ['But', 'most', 'of', 'all', 'he', 'listened', 'to', 'a', 'merry', ',', 'low', ',', 'silvery', 'laugh', 'that', 'never', 'stopped', 'but', 'went', 'on', 'and', 'on', ',', 'until', 'he', 'just', 'felt', 'as', 'if', 'he', 'must', 'laugh', 'too', '.'], ['It', 'was', 'the', 'voice', 'of', 'the', 'Laughing', 'Brook', '.'], ['And', 'as', 'Buster', 'listened', 'it', 'suddenly', 'came', 'to', 'him', 'just', 'what', 'he', 'wanted', 'for', 'breakfast', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(nltk.corpus.reader)"
      ],
      "metadata": {
        "id": "m3Ed49D8KKfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading your own Corpus"
      ],
      "metadata": {
        "id": "0JUp4Gz74ttA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import PlaintextCorpusReader\n",
        "\n",
        "corpus_root = \"YOUR FILE PATH\"\n",
        "\n",
        "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
        "\n",
        "wordlists.fileids()\n",
        "\n",
        "wordlists.words('connectives')"
      ],
      "metadata": {
        "id": "-ehBTUdj46WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More Python: Reusing Code"
      ],
      "metadata": {
        "id": "hH2WcPD8MiXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function\n",
        "\n",
        "def lexical_diversity(text):\n",
        "  return len(text) / len(set(text))\n",
        "\n",
        "lexical_diversity(emma)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkCRlGBnT_km",
        "outputId": "76e4215a-1802-4b87-9171-e48fc5b943af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24.63538599411087"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function w/ local variables\n",
        "\n",
        "def lexical_diversity(my_text_data):\n",
        "  word_count = len(my_text_data)\n",
        "  vocab_size = len(set(my_text_data))\n",
        "  diversity_score = vocab_size / word_count\n",
        "  return diversity_score\n",
        "\n",
        "lexical_diversity(emma)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfxsG-8CU51p",
        "outputId": "ff7c9912-201a-488c-ff21-f0b4f699d51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.04059201671283136"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.corpus import genesis\n",
        "nltk.download('genesis')\n",
        "kjv = genesis.words(\"english-kjv.txt\")\n",
        "lexical_diversity(kjv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAdW58PKWG7U",
        "outputId": "a05bd0f2-59bf-4ebc-98f9-c67cfbb4c872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/genesis.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06230453042623537"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lexical Resources\n",
        "\n",
        "A lexicon, or lexical resource, is a collection of words and/or phrases along with associated information such as part of speech and sense definitions. Lexical resources are secondary to texts, and are usually created and enriched with the help of texts. "
      ],
      "metadata": {
        "id": "mvFdHHYZXFEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Wordlist Corpora (NLTK includes some corpora that are nothing more than wordlists; We can use it to find unusual or mis-spelt words in a text corpus)\n",
        "* corpus of stopwords\n"
      ],
      "metadata": {
        "id": "2CarSdhJhmm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')"
      ],
      "metadata": {
        "id": "tYPvoCwMW3mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def content_fraction(text):\n",
        "  stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "  content = [w for w in text if w.lower() not in stopwords]\n",
        "  return len(content)/len(text)\n",
        "\n",
        "content_fraction(emma)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo5uec59mZh9",
        "outputId": "016f866f-ffa9-43d7-cfb4-e5e066de7349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.735240435097661"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with the help of the stopwords we can filter out over a quarter of the words of the text \n",
        "\n",
        "nltk.download('reuters')\n",
        "content_fraction(nltk.corpus.reuters.words())"
      ],
      "metadata": {
        "id": "nWK_j-7snMV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordNet is a semantically-oriented dictionary of English, similar to a traditional thesaurus but with a richer structure. NLTK includes the English WordNet, with 155,287 words and 117,659 synonym sets. We'll begin by looking at synonyms and how they are accessed in WordNet.\n",
        "\n",
        "https://wordnet.princeton.edu/"
      ],
      "metadata": {
        "id": "_7PkQBtGoHP7"
      }
    }
  ]
}